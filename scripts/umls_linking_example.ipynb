{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scispacy to Link Terms to UMLS Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an overview of the script to link terms in HAWC to UMLS entities. Run the block below to setup all of the utility functions. This is a verbatim copy of `utils.py`.\n",
    "\n",
    "You can use the following commands to setup an example environment using conda:\n",
    "\n",
    "1. `conda create --name umls python=3.7 numpy=1.18 scikit-learn=0.20.3 pandas xlrd joblib scipy spacy lxml`\n",
    "2. `conda activate umls`\n",
    "3. `conda install -c conda-forge rapidfuzz`\n",
    "4. `pip install --no-binary :all: nmslib` (optional)\n",
    "5. `pip install scispacy==0.3.0`\n",
    "6. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_sm-0.3.0.tar.gz`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Utils.\n",
    "\n",
    "@author: scott\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "from scispacy.linking import EntityLinker\n",
    "from rapidfuzz import fuzz\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "_cache = {}\n",
    "\n",
    "\n",
    "def loadUMLSLinker(max_UMLS_Returns=999, confidence=0.5,\n",
    "                   model=\"en_core_sci_sm\"):\n",
    "    \"\"\"Load model.\"\"\"\n",
    "    nlp = spacy.load(model)\n",
    "    linker = EntityLinker(resolve_abbreviations=False, name=\"umls\",\n",
    "                          max_entities_per_mention=max_UMLS_Returns,\n",
    "                          threshold=confidence)\n",
    "    nlp.add_pipe(linker)\n",
    "    return (linker, nlp)\n",
    "\n",
    "\n",
    "def loadDocs(series, nlp):\n",
    "    \"\"\"Run nlp on text.\"\"\"\n",
    "    series_list = series.drop_duplicates().apply(transformComma, entity=True)\n",
    "    text_list = set([j.strip() for i in series_list for j in i if len(j) > 2])\n",
    "    docs_dict = {text: nlp(text) for text in text_list}\n",
    "    return docs_dict\n",
    "\n",
    "\n",
    "def transformComma(text: str, entity: bool) -> list:\n",
    "    \"\"\"Remove commas and reverse string.\"\"\"\n",
    "    if text.endswith('.'):\n",
    "        return [text]\n",
    "    text_split = [str(i).strip() for i in text.split(\",\")]\n",
    "    text_flipped = [str(i).strip() for i in text_split[::-1]]\n",
    "    comma_removed = \" \".join(text_split)\n",
    "    comma_reversed = \" \".join(text_flipped)\n",
    "    comb = [comma_reversed, comma_removed]\n",
    "    comb.append(text)\n",
    "    if entity:\n",
    "        comb += text_split[:1]\n",
    "    return comb\n",
    "\n",
    "\n",
    "def umls_filter(x, string=False, max_results=5, strict=True) -> str:\n",
    "    \"\"\"Convert umls list of dicts to string.\"\"\"\n",
    "    text = x.iloc[0]\n",
    "    umlsList = x.iloc[1]\n",
    "\n",
    "    for i in range(2, len(x.index)):\n",
    "        for k in x.iloc[i]:\n",
    "            if k['cui'] not in [j['cui'] for j in umlsList]:\n",
    "                umlsList.append(k)\n",
    "\n",
    "    if len(umlsList) == 0:\n",
    "        return '' if string else []\n",
    "\n",
    "    text_split = [str(i).strip().lower() for i in text.split(\",\")\n",
    "                  if len(i.strip()) > 2]\n",
    "    text_transformed = [i.lower() for i in transformComma(text, False)]\n",
    "    tiers = [[] for i in range(len(text_split) + 2)]\n",
    "\n",
    "    for val in umlsList:\n",
    "        syns = [i.lower() for i in val['synonyms']] + [val['name'].lower()]\n",
    "        score_d = {j:\n",
    "                   np.median([fuzz.token_sort_ratio(j, i) for i in syns])/100\n",
    "                   for j in set(text_transformed + text_split)}\n",
    "        added = False\n",
    "\n",
    "        for i in text_transformed:\n",
    "            if i in syns:\n",
    "                tiers[0].append((val, score_d[i]))\n",
    "                added = True\n",
    "                break\n",
    "        if added:\n",
    "            continue\n",
    "\n",
    "        for n, i in enumerate(text_split):\n",
    "            if i in syns:\n",
    "                tiers[n+1].append((val, score_d[i]/(n+1)/len(text_split)))\n",
    "                added = True\n",
    "                break\n",
    "        if added:\n",
    "            continue\n",
    "\n",
    "        comb_score = np.median(\n",
    "            [score_d[i]/(n+1)/len(text_split)\n",
    "             for n, i in enumerate(text_split)])\n",
    "\n",
    "        tiers[-1].append((val, comb_score))\n",
    "\n",
    "    results_list = []\n",
    "    for n, i in enumerate(tiers):\n",
    "        if strict and n == len(tiers)-1 and len(results_list) > 0:\n",
    "            break\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        results_list += sorted(i, key=lambda x: x[1], reverse=True)\n",
    "        if n == 0:\n",
    "            break\n",
    "    results_list2 = sorted(results_list, key=lambda x: x[1], reverse=True)\n",
    "    if results_list != results_list2:\n",
    "        pass\n",
    "\n",
    "    results_list = results_list[:max_results]\n",
    "\n",
    "    if string:\n",
    "        return ' // '.join(\n",
    "            [i[0]['name'] + ' (' + i[0]['cui'] + '; ' +\n",
    "             ', '.join(i[0]['tuis']) + '; ' + str(i[1]) + ')'\n",
    "             for i in results_list])\n",
    "    else:\n",
    "        return results_list\n",
    "\n",
    "\n",
    "def breakIntoSpansAndUMLS(text, docs_dict, linker,\n",
    "                          tuiFilter=None,\n",
    "                          confidence=0.5,\n",
    "                          RequireNonObsoleteDef=False,\n",
    "                          entity_break=False,\n",
    "                          cui_limit=5,\n",
    "                          cache_key=None):\n",
    "    \"\"\"Read entities and assign cui.\"\"\"\n",
    "    if cache_key is not None:\n",
    "        if cache_key not in _cache:\n",
    "            _cache[cache_key] = {}\n",
    "        elif text in _cache[cache_key]:\n",
    "            return _cache[cache_key][text]\n",
    "    confidence = 0 if confidence is None else confidence\n",
    "    cui_limit = 999 if cui_limit is None else cui_limit\n",
    "    docs = [docs_dict[text_transformed.strip()]\n",
    "            for text_transformed in transformComma(text, entity_break)\n",
    "            if len(text_transformed) > 2]\n",
    "    if entity_break:\n",
    "        entities = [i for doc in docs for i in list(doc.ents)]\n",
    "    else:\n",
    "        entities = [doc[:] for doc in docs]\n",
    "    save_cui = {}\n",
    "    for entity in entities:\n",
    "        last_score = 1\n",
    "        count = 0\n",
    "        for umls_ent in entity._.umls_ents:\n",
    "            umls_score = round(umls_ent[1], 4)\n",
    "            if umls_ent[0] in save_cui:\n",
    "                if save_cui[umls_ent[0]]['score'] < umls_score:\n",
    "                    save_cui[umls_ent[0]]['score'] = umls_score\n",
    "                continue\n",
    "            if umls_score < confidence:\n",
    "                break\n",
    "            if count >= cui_limit and umls_score < last_score:\n",
    "                break\n",
    "            umls_Code = linker.umls.cui_to_entity[umls_ent[0]]\n",
    "            TUI = umls_Code[3]\n",
    "            UMLS_Def = umls_Code[4]\n",
    "            if RequireNonObsoleteDef and \\\n",
    "                    (UMLS_Def is None or \"OBSOLETE\" in UMLS_Def):\n",
    "                continue\n",
    "            if tuiFilter is None or [i for i in TUI if i in tuiFilter]:\n",
    "                umls_dict = {'name': umls_Code[1],\n",
    "                             'cui': umls_ent[0],\n",
    "                             'score': umls_score,\n",
    "                             'synonyms': umls_Code[2],\n",
    "                             'tuis': umls_Code[3],\n",
    "                             'description': umls_Code[4],\n",
    "                             }\n",
    "                count += 1\n",
    "                last_score = umls_score\n",
    "                save_cui[umls_ent[0]] = umls_dict\n",
    "    umlsList = [val for key, val in save_cui.items()]\n",
    "\n",
    "    umls_sorted = sorted(umlsList, key=lambda x: x['score'], reverse=True)\n",
    "    umls_limited = [i for i in umls_sorted if i['score'] >=\n",
    "                    umls_sorted[min(cui_limit,\n",
    "                                    len(umls_sorted))-1]['score']]\n",
    "    umlsList = umls_limited\n",
    "\n",
    "    _cache[cache_key][text] = umlsList\n",
    "\n",
    "    return umlsList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start linking terms. First, load a DataFrame of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"HAWC-Ontologies-July2020v2.xlsx\",\n",
    "                   sheet_name=\"Preferred Terms List-July 2020\",\n",
    "                   usecols=\"A:C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to load the models. If this is the first time you run the command, a bunch of data will need to be downloaded. You can use the parameters to change which scispacy model you want loaded, as well as info like the minimum confidence interval. Loading the models will take about a minute once everyting is downloaded, but you'll only need to do it once. I would highly recommend using a computer with 32 GB of memory for this, although you can probably get by with 16 if you close Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker, nlp = loadUMLSLinker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also feed all of the text into the model beforehand to speed things up. The input is a series which contains every string that you want to map to a UMLS entity. This method can be easily multithreaded if you have more than a few thousand terms, otherwise it's probably not worth it. Feeding in a series with around 3,000 unique strings took about a minute to run in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loadDocs(pd.concat(\n",
    "    [df['endpoint-organ'], df['endpoint-system'], df['endpoint-name']]),\n",
    "    nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the computationally heavy steps are already complete. From this point on, every step should be pretty much instantaneous, including the tiered analysis. The code below is an example of how to match terms. In this example, the `endpoint-organ` column is fed into the mapping method, with the output being a new series. `linker`, `docs`, and `nlp` need to be fed into the method along with the series. There are a few other parameters you can add:\n",
    "\n",
    "- `max_results`: Maximum number of results to return for each entity found in the string. Default is 10.\n",
    "- `confidence`: Minimum confidence score for a result to be returned. Default is 0.5.\n",
    "- `tuis`: An iterable containing valid TUIs. Default is `None` (all TUIs valid).\n",
    "- `entity_break`: Bool. If `false`, the entire search string must match an entity. If `true`, entities can be found on substrings.\n",
    "- `RequireNonObsoleteDef`: Bool. Whether to allow abselete entities (based on definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [{'name': 'Serum', 'cui': 'C0229671', 'score':...\n",
      "1    [{'name': 'Serum', 'cui': 'C0229671', 'score':...\n",
      "2    [{'name': 'Vasculature', 'cui': 'C3714653', 's...\n",
      "3    [{'name': 'Heart', 'cui': 'C0018787', 'score':...\n",
      "4    [{'name': 'Heart', 'cui': 'C0018787', 'score':...\n",
      "Name: endpoint-organ_UMLS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def run_tier(series: pd.Series, linker, docs_dict, nlp,\n",
    "             max_results: int = 10,\n",
    "             confidence: float = 0.5, tuis=None, entity_break=False,\n",
    "             RequireNonObsoleteDef=False):\n",
    "    \"\"\"Get entity from series.\"\"\"\n",
    "    cache_key = hash(\n",
    "        (nlp,\n",
    "         linker,\n",
    "         max_results if max_results is not None else '',\n",
    "         confidence if confidence is not None else '',\n",
    "         ''.join(tuis) if tuis is not None else '',\n",
    "         entity_break, RequireNonObsoleteDef,))\n",
    "    new_series = series.apply(breakIntoSpansAndUMLS,\n",
    "                              docs_dict=docs_dict,\n",
    "                              linker=linker,\n",
    "                              tuiFilter=tuis,\n",
    "                              RequireNonObsoleteDef=RequireNonObsoleteDef,\n",
    "                              confidence=confidence,\n",
    "                              entity_break=entity_break,\n",
    "                              cui_limit=max_results,\n",
    "                              cache_key=cache_key,\n",
    "                              )\n",
    "    return new_series\n",
    "\n",
    "df[\"endpoint-organ_UMLS\"] = \\\n",
    "    run_tier(df[\"endpoint-organ\"], linker, docs, nlp,\n",
    "             1, 0.5)\n",
    "print(df[\"endpoint-organ_UMLS\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another method which helps organize the outputs. It can help to filter out some bad outputs. `run_tier` returns a list of UMLS entities for each entry. This filtering function can also convert this list into a string by setting `string` to `True`. The DataFrame that the filter function gets applied to should have at least 2 columns. The first one should be original strings, and the second one should be the list of UMLS entities. More columns of UMLS entities can be added (they will be combined with the second column).\n",
    "\n",
    "If you want to limit the maximum number of results that are returned (e.g. to 1), you can use the `max_results` parameter.\n",
    "\n",
    "This filtering function returns a list of tuples in the form `(results, score)`, where the score is calculated by comparing the average similarity of a part of the text to each synonym, divided by number of comma-separated parts that the string has and location in that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Serum (C0229671; T031; 0.6666666666666665) // ...\n",
      "1    Serum (C0229671; T031; 0.6666666666666665) // ...\n",
      "2    Vasculature (C3714653; T017; 1.0) // Blood sup...\n",
      "3          Heart (C0018787; T023; 0.36363636363636365)\n",
      "4          Heart (C0018787; T023; 0.36363636363636365)\n",
      "Name: endpoint-organ_UMLS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['endpoint-organ_UMLS'] = pd.concat(\n",
    "    [df[\"endpoint-organ\"], df[\"endpoint-organ_UMLS\"]], axis=1) \\\n",
    "    .apply(umls_filter, axis=1, result_type='reduce', string=True)\n",
    "print(df[\"endpoint-organ_UMLS\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the other thing we can do is run a tiered analysis, where we run `run_tier` with different parameters that have descending strictness. Here is an example of that. You can change the parameters of each tier based on the dataset, or add and remove tiers. All of the indexing allows the function to stop running on a value once it finds a match.\n",
    "\n",
    "This specific function has 5 tiers, with descending minimum scores. The last tier also allows sets `entity_break=True`, and runs on every value even if a match was already found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tier 1 on endpoint-name\n",
      "255 names mapped\n",
      "Running tier 2 on endpoint-name\n",
      "263 names mapped\n",
      "Running tier 3 on endpoint-name\n",
      "628 names mapped\n",
      "Running tier 4 on endpoint-name\n",
      "9 names mapped\n",
      "Running tier 7 on endpoint-name\n",
      "1421 names mapped\n",
      "Running tier 8 on endpoint-name\n",
      "Done, 179 names not mapped\n",
      "Filtering and calculating scores...\n",
      "Done\n",
      "         endpoint-name                                 endpoint-name_UMLS  \\\n",
      "0   Fatty Acid Balance  [({'name': 'Fatty Acids', 'cui': 'C0015684', '...   \n",
      "1  Fatty Acids, Total   [({'name': 'Fatty Acids', 'cui': 'C0015684', '...   \n",
      "2      Embolus, Aortic  [({'name': 'aortic embolus', 'cui': 'C0741165'...   \n",
      "3       Cardiomyopathy  [({'name': 'Cardiomyopathies', 'cui': 'C087854...   \n",
      "4     Heart, Neoplasms  [({'name': 'Heart Neoplasm', 'cui': 'C0018809'...   \n",
      "\n",
      "  endpoint-name_Tier  \n",
      "0             Tier 7  \n",
      "1             Tier 2  \n",
      "2             Tier 2  \n",
      "3             Tier 1  \n",
      "4             Tier 1  \n"
     ]
    }
   ],
   "source": [
    "class TuiFilters:\n",
    "    \"\"\"Tui list for rescricting mappings.\"\"\"\n",
    "\n",
    "    tuis_3 = {\n",
    "        \"T053\", \"T054\", \"T055\", \"T017\", \"T018\", \"T021\", \"T022\", \"T023\", \"T024\",\n",
    "        \"T025\", \"T026\", \"T029\", \"T030\", \"T031\", \"T109\", \"T114\", \"T116\", \"T121\",\n",
    "        \"T123\", \"T125\", \"T126\", \"T127\", \"T129\", \"T131\", \"T192\", \"T196\", \"T079\",\n",
    "        \"T080\", \"T081\", \"T082\", \"T102\", \"T169\", \"T185\", \"T034\", \"T038\", \"T032\",\n",
    "        \"T039\", \"T040\", \"T041\", \"T042\", \"T043\", \"T044\", \"T045\", \"T201\", \"T019\",\n",
    "        \"T020\", \"T033\", \"T037\", \"T046\", \"T047\", \"T048\", \"T049\", \"T050\", \"T184\",\n",
    "        \"T190\", \"T191\", \"T059\",\n",
    "    }\n",
    "    tuis = {\n",
    "        \"T053\", \"T054\", \"T055\", \"T017\", \"T018\", \"T021\", \"T022\", \"T023\", \"T024\",\n",
    "        \"T025\", \"T026\", \"T029\", \"T030\", \"T031\", \"T109\", \"T114\", \"T116\", \"T121\",\n",
    "        \"T123\", \"T125\", \"T126\", \"T127\", \"T129\", \"T131\", \"T192\", \"T196\", \"T079\",\n",
    "        \"T080\", \"T081\", \"T082\", \"T102\", \"T169\", \"T185\", \"T034\", \"T038\", \"T032\",\n",
    "        \"T039\", \"T040\", \"T041\", \"T042\", \"T043\", \"T044\", \"T045\", \"T201\", \"T019\",\n",
    "        \"T020\", \"T033\", \"T037\", \"T046\", \"T047\", \"T048\", \"T049\", \"T050\", \"T184\",\n",
    "        \"T190\", \"T191\",\n",
    "    }\n",
    "\n",
    "def run_tiered_analysis(\n",
    "        series: pd.Series, linker, nlp, docs, max_results=10, string=False,\n",
    "        strict_matching=True,\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"Run a tiered analysis on endpoint-name.\"\"\"\n",
    "    cui_limit = 10 if max_results < 10 else max_results\n",
    "\n",
    "    print(f\"Running tier 1 on {series.name}\")\n",
    "    t1 = run_tier(\n",
    "        series, linker, docs, nlp, cui_limit, 0.98, TuiFilters.tuis,\n",
    "        RequireNonObsoleteDef=True)\n",
    "    nfound = len(t1.loc[t1.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 2 on {series.name}\")\n",
    "    t2 = run_tier(\n",
    "        series.loc[t1.loc[t1.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.85, TuiFilters.tuis)\n",
    "    nfound = len(t2.loc[t2.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 3 on {series.name}\")\n",
    "    t3 = run_tier(\n",
    "        series.loc[t2.loc[t2.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.70, TuiFilters.tuis_3)\n",
    "    nfound = len(t3.loc[t3.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 4 on {series.name}\")\n",
    "    t4 = run_tier(\n",
    "        series.loc[t3.loc[t3.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.85, None)\n",
    "    nfound = len(t4.loc[t4.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 7 on {series.name}\")\n",
    "    t7 = run_tier(\n",
    "        series.loc[t4.loc[t4.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.85, TuiFilters.tuis, entity_break=True)\n",
    "    nfound = len(t7.loc[t7.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 8 on {series.name}\")\n",
    "    # this tier will run for value, unlike the ones above\n",
    "    t8 = run_tier(series, linker, docs, nlp,\n",
    "                  cui_limit, 0.85, TuiFilters.tuis, entity_break=True)\n",
    "    t8.name = series.name + 'Tier8'\n",
    "\n",
    "    comb_tiers = pd.concat([pd.concat(\n",
    "                                [t1, pd.Series('Tier 1', index=t1.index)],\n",
    "                                axis=1).loc[t1.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t2, pd.Series('Tier 2', index=t2.index)],\n",
    "                                axis=1).loc[t2.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t3, pd.Series('Tier 3', index=t3.index)],\n",
    "                                axis=1).loc[t3.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t4, pd.Series('Tier 4', index=t4.index)],\n",
    "                                axis=1).loc[t4.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t7, pd.Series('Tier 7', index=t7.index)],\n",
    "                                axis=1).loc[t7.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t7, pd.Series('', index=t7.index)],\n",
    "                                axis=1).loc[t7.map(len) == 0],\n",
    "                            ]).sort_index()\n",
    "    comb_tiers.columns = [series.name + '_UMLS', series.name + '_Tier']\n",
    "    # df_new = pd.concat([series, comb_tiers], axis=1)\n",
    "    df_new = comb_tiers\n",
    "\n",
    "    notfound = len(t7.loc[t7.map(len) == 0])\n",
    "    print(f'Done, {notfound} names not mapped')\n",
    "\n",
    "    print('Filtering and calculating scores...')\n",
    "    df_new[series.name + '_UMLS'] = pd.concat(\n",
    "        [series, df_new[series.name + '_UMLS'], t8], axis=1) \\\n",
    "        .apply(umls_filter, axis=1, result_type='reduce', string=string,\n",
    "               max_results=max_results, strict=strict_matching)\n",
    "\n",
    "    print('Done')\n",
    "    return df_new\n",
    "\n",
    "df_tier = run_tiered_analysis(df[\"endpoint-name\"], linker, nlp, docs)\n",
    "df_comb = pd.concat([df[[\"endpoint-name\"]], df_tier], axis=1)\n",
    "print(df_comb.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

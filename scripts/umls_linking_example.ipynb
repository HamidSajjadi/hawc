{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scispacy to Link Terms to UMLS Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an overview of the script to link terms in HAWC to UMLS entities. Run the block below to setup all of the utility functions. This is a verbatim copy of `utils.py`.\n",
    "\n",
    "You can use the following commands to setup an example environment using conda:\n",
    "\n",
    "1. `conda create --name umls python=3.7 numpy=1.18 scikit-learn=0.20.3 pandas xlrd joblib scipy spacy lxml`\n",
    "2. `conda activate umls`\n",
    "3. `pip install --no-binary :all: nmslib` (optional)\n",
    "4. `pip install scispacy`\n",
    "5. `pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_sm-0.2.5.tar.gz`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Utils.\n",
    "\n",
    "@author: scott\n",
    "\"\"\"\n",
    "\n",
    "import spacy\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "\n",
    "_cache = {}\n",
    "\n",
    "\n",
    "def loadUMLSLinker(max_UMLS_Returns=999, confidence=0.5,\n",
    "                   model=\"en_core_sci_sm\"):\n",
    "    \"\"\"Load model.\"\"\"\n",
    "    nlp = spacy.load(model)\n",
    "    linker = EntityLinker(resolve_abbreviations=False, name=\"umls\",\n",
    "                          max_entities_per_mention=max_UMLS_Returns,\n",
    "                          threshold=confidence)\n",
    "    nlp.add_pipe(linker)\n",
    "    return (linker, nlp)\n",
    "\n",
    "\n",
    "def loadDocs(series, nlp):\n",
    "    \"\"\"Run nlp on text.\"\"\"\n",
    "    series_list = series.drop_duplicates().apply(transformComma, entity=True)\n",
    "    text_list = set([j.strip() for i in series_list for j in i if len(j) > 2])\n",
    "    docs_dict = {text: nlp(text) for text in text_list}\n",
    "    return docs_dict\n",
    "\n",
    "\n",
    "def transformComma(text: str, entity: bool) -> list:\n",
    "    \"\"\"Remove commas and reverse string.\"\"\"\n",
    "    if text.endswith('.'):\n",
    "        return [text]\n",
    "    text_split = [str(i).strip() for i in text.split(\",\")]\n",
    "    text_flipped = [str(i).strip() for i in text_split[::-1]]\n",
    "    comma_removed = \" \".join(text_split)\n",
    "    comma_reversed = \" \".join(text_flipped)\n",
    "    comb = [comma_reversed, comma_removed]\n",
    "    comb.append(text)\n",
    "    if entity:\n",
    "        comb += text_split[:1]\n",
    "    return comb\n",
    "\n",
    "\n",
    "def umls_filter(x, string=True) -> str:\n",
    "    \"\"\"Convert umls list of dicts to string.\"\"\"\n",
    "    text = x.iloc[0]\n",
    "    umlsList = x.iloc[1]\n",
    "\n",
    "    for i in range(2, len(x.index)):\n",
    "        umlsList_temp = x.iloc[i]\n",
    "        for i in umlsList_temp:\n",
    "            if i['cui'] not in [j['cui'] for j in umlsList]:\n",
    "                umlsList.append(i)\n",
    "\n",
    "    base = '' if string else []\n",
    "    if len(umlsList) == 0:\n",
    "        return base\n",
    "\n",
    "    text_split = [str(i).strip().lower() for i in text.split(\",\")\n",
    "                  if len(i.strip()) > 2]\n",
    "    text_transformed = [i.lower() for i in transformComma(text, False)]\n",
    "    tiers = [[] for i in range(len(text_split) + 2)]\n",
    "\n",
    "    for val in umlsList:\n",
    "        syns = [i.lower() for i in val['synonyms']] + [val['name'].lower()]\n",
    "        added = False\n",
    "\n",
    "        for i in text_transformed:\n",
    "            if i in syns:\n",
    "                tiers[0].append(val)\n",
    "                added = True\n",
    "                break\n",
    "        if added:\n",
    "            continue\n",
    "\n",
    "        for n, i in enumerate(text_split):\n",
    "            if i in syns:\n",
    "                tiers[n+1].append(val)\n",
    "                added = True\n",
    "                break\n",
    "        if added:\n",
    "            continue\n",
    "\n",
    "        tiers[-1].append(val)\n",
    "\n",
    "    results_list = []\n",
    "    for n, i in enumerate(tiers):\n",
    "        if n == len(tiers)-1 and len(results_list) > 0:\n",
    "            break\n",
    "        if len(i) == 0:\n",
    "            continue\n",
    "        results_list += sorted(i, key=lambda x: x['score'], reverse=True)\n",
    "        if n == 0:\n",
    "            break\n",
    "\n",
    "    if string:\n",
    "        return ' // '.join(\n",
    "            [i['name'] + ' (' + i['cui'] + '; ' + ', '.join(i['tuis']) + ')'\n",
    "             for i in results_list])\n",
    "    else:\n",
    "        return results_list\n",
    "\n",
    "\n",
    "def breakIntoSpansAndUMLS(text, docs_dict, linker,\n",
    "                          tuiFilter=None,\n",
    "                          confidence=0.5,\n",
    "                          RequireNonObsoleteDef=False,\n",
    "                          entity_break=False,\n",
    "                          cui_limit=5,\n",
    "                          cache_key=None):\n",
    "    \"\"\"Read entities and assign cui.\"\"\"\n",
    "    if cache_key is not None:\n",
    "        if cache_key not in _cache:\n",
    "            _cache[cache_key] = {}\n",
    "        elif text in _cache[cache_key]:\n",
    "            return _cache[cache_key][text]\n",
    "    confidence = 0 if confidence is None else confidence\n",
    "    cui_limit = 999 if cui_limit is None else cui_limit\n",
    "    docs = [docs_dict[text_transformed.strip()]\n",
    "            for text_transformed in transformComma(text, entity_break)\n",
    "            if len(text_transformed) > 2]\n",
    "    if entity_break:\n",
    "        entities = [i for doc in docs for i in list(doc.ents)]\n",
    "    else:\n",
    "        entities = [doc[:] for doc in docs]\n",
    "    save_cui = {}\n",
    "    for entity in entities:\n",
    "        last_score = 1\n",
    "        count = 0\n",
    "        for umls_ent in entity._.umls_ents:\n",
    "            umls_score = round(umls_ent[1], 4)\n",
    "            if umls_ent[0] in save_cui:\n",
    "                if save_cui[umls_ent[0]]['score'] < umls_score:\n",
    "                    save_cui[umls_ent[0]]['score'] = umls_score\n",
    "                continue\n",
    "            if umls_score < confidence:\n",
    "                break\n",
    "            if count >= cui_limit and umls_score < last_score:\n",
    "                break\n",
    "            umls_Code = linker.umls.cui_to_entity[umls_ent[0]]\n",
    "            TUI = umls_Code[3]\n",
    "            UMLS_Def = umls_Code[4]\n",
    "            if RequireNonObsoleteDef and \\\n",
    "                    (UMLS_Def is None or \"OBSOLETE\" in UMLS_Def):\n",
    "                continue\n",
    "            if tuiFilter is None or [i for i in TUI if i in tuiFilter]:\n",
    "                umls_dict = {'name': umls_Code[1],\n",
    "                             'cui': umls_ent[0],\n",
    "                             'score': umls_score,\n",
    "                             'synonyms': umls_Code[2],\n",
    "                             'tuis': umls_Code[3],\n",
    "                             'description': umls_Code[4],\n",
    "                             }\n",
    "                count += 1\n",
    "                last_score = umls_score\n",
    "                save_cui[umls_ent[0]] = umls_dict\n",
    "    umlsList = [val for key, val in save_cui.items()]\n",
    "\n",
    "    umls_sorted = sorted(umlsList, key=lambda x: x['score'], reverse=True)\n",
    "    umls_limited = [i for i in umls_sorted if i['score'] >=\n",
    "                    umls_sorted[min(cui_limit,\n",
    "                                    len(umls_sorted))-1]['score']]\n",
    "    umlsList = umls_limited\n",
    "\n",
    "    _cache[cache_key][text] = umlsList\n",
    "\n",
    "    return umlsList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start linking terms. First, load a DataFrame of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"HAWC-Ontologies-July2020v2.xlsx\",\n",
    "                   sheet_name=\"Preferred Terms List-July 2020\",\n",
    "                   usecols=\"A:C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to load the models. If this is the first time you run the command, a bunch of data will need to be downloaded. You can use the parameters to change which scispacy model you want loaded, as well as info like the minimum confidence interval. Loading the models will take about a minute once everyting is downloaded, but you'll only need to do it once. I would highly recommend using a computer with 32 GB of memory for this, although you can probably get by with 16 if you close Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker, nlp = loadUMLSLinker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also feed all of the text into the model beforehand to speed things up. The input is a series which contains every string that you want to map to a UMLS entity. This method can be easily multithreaded if you have more than a few thousand terms, otherwise it's probably not worth it. Feeding in a series with around 3,000 unique strings took about a minute to run in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loadDocs(pd.concat(\n",
    "    [df['endpoint-organ'], df['endpoint-system'], df['endpoint-name']]),\n",
    "    nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the computationally heavy steps are already complete. From this point on, every step should be pretty much instantaneous, including the tiered analysis. The code below is an example of how to match terms. In this example, the `endpoint-organ` column is fed into the mapping method, with the output being a new series. `linker`, `docs`, and `nlp` need to be fed into the method along with the series. There are a few other parameters you can add:\n",
    "\n",
    "- `max_results`: Maximum number of results to return for each entity found in the string. Default is 10.\n",
    "- `confidence`: Minimum confidence score for a result to be returned. Default is 0.5.\n",
    "- `tuis`: An iterable containing valid TUIs. Default is `None` (all TUIs valid).\n",
    "- `entity_break`: Bool. If `false`, the entire search string must match an entity. If `true`, entities can be found on substrings.\n",
    "- `RequireNonObsoleteDef`: Bool. Whether to allow abselete entities (based on definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [{'name': 'Serum', 'cui': 'C0229671', 'score':...\n",
      "1    [{'name': 'Serum', 'cui': 'C0229671', 'score':...\n",
      "2    [{'name': 'Blood supply aspects', 'cui': 'C000...\n",
      "3    [{'name': 'Heart', 'cui': 'C0018787', 'score':...\n",
      "4    [{'name': 'Heart', 'cui': 'C0018787', 'score':...\n",
      "Name: endpoint-organ_UMLS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def run_tier(series: pd.Series, linker, docs_dict, nlp,\n",
    "             max_results: int = 10,\n",
    "             confidence: float = 0.5, tuis=None, entity_break=False,\n",
    "             RequireNonObsoleteDef=False):\n",
    "    \"\"\"Get entity from series.\"\"\"\n",
    "    cache_key = hash(\n",
    "        (nlp,\n",
    "         linker,\n",
    "         max_results if max_results is not None else '',\n",
    "         confidence if confidence is not None else '',\n",
    "         ''.join(tuis) if tuis is not None else '',\n",
    "         entity_break, RequireNonObsoleteDef,))\n",
    "    new_series = series.apply(breakIntoSpansAndUMLS,\n",
    "                              docs_dict=docs_dict,\n",
    "                              linker=linker,\n",
    "                              tuiFilter=tuis,\n",
    "                              RequireNonObsoleteDef=RequireNonObsoleteDef,\n",
    "                              confidence=confidence,\n",
    "                              entity_break=entity_break,\n",
    "                              cui_limit=max_results,\n",
    "                              cache_key=cache_key,\n",
    "                              )\n",
    "    return new_series\n",
    "\n",
    "df[\"endpoint-organ_UMLS\"] = \\\n",
    "    run_tier(df[\"endpoint-organ\"], linker, docs, nlp,\n",
    "             1, 0.5)\n",
    "print(df[\"endpoint-organ_UMLS\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another method which helps organize the outputs. It can help to filter out some bad outputs. `run_tier` returns a list of UMLS entities for each entry. This filtering function can also convert this list into a string by setting `string` to `True`. The DataFrame that the filter function gets applied to should have at least 2 columns. The first one should be original strings, and the second one should be the list of UMLS entities. More columns of UMLS entities can be added (they will be combined with the second column).\n",
    "\n",
    "If you want to limit the maximum number of results that are returned (e.g. to 1), this would be the place to add that functionality (rather than above when calling `run_tier`). A list or string is returned by this function. In general, the position in that list returned by this function indicates the quality of the match. That is to say, the first result is the best one. This isn't 100% true (since two matches can be basically tied in importance with how the code is written right now) but is a good guideline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Serum (C0229671; T031) // Specimen Type - Seru...\n",
      "1    Serum (C0229671; T031) // Specimen Type - Seru...\n",
      "2    Blood supply aspects (C0005839; T080) // Vascu...\n",
      "3                               Heart (C0018787; T023)\n",
      "4                               Heart (C0018787; T023)\n",
      "Name: endpoint-organ_UMLS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['endpoint-organ_UMLS'] = pd.concat(\n",
    "    [df[\"endpoint-organ\"], df[\"endpoint-organ_UMLS\"]], axis=1) \\\n",
    "    .apply(umls_filter, axis=1, result_type='reduce', string=True)\n",
    "print(df[\"endpoint-organ_UMLS\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the other thing we can do is run a tiered analysis, where we run `run_tier` with different parameters that have descending strictness. Here is an example of that. You can change the parameters of each tier based on the dataset, or add and remove tiers. All of the indexing allows the function to stop running on a value once it finds a match.\n",
    "\n",
    "This specific function has 5 tiers, with descending minimum scores. The last tier also allows sets `entity_break=True`, and runs on every value even if a match was already found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tier 1 on endpoint-name\n",
      "215 names mapped\n",
      "Running tier 2 on endpoint-name\n",
      "170 names mapped\n",
      "Running tier 3 on endpoint-name\n",
      "406 names mapped\n",
      "Running tier 4 on endpoint-name\n",
      "6 names mapped\n",
      "Running tier 7 on endpoint-name\n",
      "1776 names mapped\n",
      "Running tier 8 on endpoint-name\n",
      "Done, 182 names not mapped\n",
      "         endpoint-name                 endpoint-name_UMLS endpoint-name_Tier\n",
      "0   Fatty Acid Balance       Fatty Acids (C0015684; T109)             Tier 7\n",
      "1  Fatty Acids, Total        Fatty Acids (C0015684; T109)             Tier 2\n",
      "2      Embolus, Aortic    aortic embolus (C0741165; T046)             Tier 2\n",
      "3       Cardiomyopathy  Cardiomyopathies (C0878544; T047)             Tier 1\n",
      "4     Heart, Neoplasms    Heart Neoplasm (C0018809; T191)             Tier 1\n"
     ]
    }
   ],
   "source": [
    "class TuiFilters:\n",
    "    \"\"\"Tui list for rescricting mappings.\"\"\"\n",
    "\n",
    "    tuis_3 = {\n",
    "        \"T053\", \"T054\", \"T055\", \"T017\", \"T018\", \"T021\", \"T022\", \"T023\", \"T024\",\n",
    "        \"T025\", \"T026\", \"T029\", \"T030\", \"T031\", \"T109\", \"T114\", \"T116\", \"T121\",\n",
    "        \"T123\", \"T125\", \"T126\", \"T127\", \"T129\", \"T131\", \"T192\", \"T196\", \"T079\",\n",
    "        \"T080\", \"T081\", \"T082\", \"T102\", \"T169\", \"T185\", \"T034\", \"T038\", \"T032\",\n",
    "        \"T039\", \"T040\", \"T041\", \"T042\", \"T043\", \"T044\", \"T045\", \"T201\", \"T019\",\n",
    "        \"T020\", \"T033\", \"T037\", \"T046\", \"T047\", \"T048\", \"T049\", \"T050\", \"T184\",\n",
    "        \"T190\", \"T191\", \"T059\",\n",
    "    }\n",
    "    tuis = {\n",
    "        \"T053\", \"T054\", \"T055\", \"T017\", \"T018\", \"T021\", \"T022\", \"T023\", \"T024\",\n",
    "        \"T025\", \"T026\", \"T029\", \"T030\", \"T031\", \"T109\", \"T114\", \"T116\", \"T121\",\n",
    "        \"T123\", \"T125\", \"T126\", \"T127\", \"T129\", \"T131\", \"T192\", \"T196\", \"T079\",\n",
    "        \"T080\", \"T081\", \"T082\", \"T102\", \"T169\", \"T185\", \"T034\", \"T038\", \"T032\",\n",
    "        \"T039\", \"T040\", \"T041\", \"T042\", \"T043\", \"T044\", \"T045\", \"T201\", \"T019\",\n",
    "        \"T020\", \"T033\", \"T037\", \"T046\", \"T047\", \"T048\", \"T049\", \"T050\", \"T184\",\n",
    "        \"T190\", \"T191\",\n",
    "    }\n",
    "\n",
    "def run_tiered_analysis(\n",
    "        series: pd.Series, linker, nlp, docs, max_results=1, force_limit=True,\n",
    "        ) -> pd.DataFrame:\n",
    "    \"\"\"Run a tiered analysis on endpoint-name.\"\"\"\n",
    "    cui_limit = 10  # don't change this\n",
    "\n",
    "    print(f\"Running tier 1 on {series.name}\")\n",
    "    t1 = run_tier(\n",
    "        series, linker, docs, nlp, cui_limit, 0.98, TuiFilters.tuis,\n",
    "        RequireNonObsoleteDef=True)\n",
    "    nfound = len(t1.loc[t1.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 2 on {series.name}\")\n",
    "    t2 = run_tier(\n",
    "        series.loc[t1.loc[t1.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.85, TuiFilters.tuis)\n",
    "    nfound = len(t2.loc[t2.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 3 on {series.name}\")\n",
    "    t3 = run_tier(\n",
    "        series.loc[t2.loc[t2.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.70, TuiFilters.tuis_3)\n",
    "    nfound = len(t3.loc[t3.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 4 on {series.name}\")\n",
    "    t4 = run_tier(\n",
    "        series.loc[t3.loc[t3.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.85, None)\n",
    "    nfound = len(t4.loc[t4.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 7 on {series.name}\")\n",
    "    t7 = run_tier(\n",
    "        series.loc[t4.loc[t4.map(len) == 0].index], linker, docs, nlp,\n",
    "        cui_limit, 0.85, TuiFilters.tuis, entity_break=True)\n",
    "    nfound = len(t7.loc[t7.map(len) != 0])\n",
    "    print(f'{nfound} names mapped')\n",
    "\n",
    "    print(f\"Running tier 8 on {series.name}\")\n",
    "    # this tier will run for value, unlike the ones above\n",
    "    t8 = run_tier(series, linker, docs, nlp,\n",
    "                  cui_limit, 0.85, TuiFilters.tuis, entity_break=True)\n",
    "    t8.name = series.name + 'Tier8'\n",
    "\n",
    "    comb_tiers = pd.concat([pd.concat(\n",
    "                                [t1, pd.Series('Tier 1', index=t1.index)],\n",
    "                                axis=1).loc[t1.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t2, pd.Series('Tier 2', index=t2.index)],\n",
    "                                axis=1).loc[t2.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t3, pd.Series('Tier 3', index=t3.index)],\n",
    "                                axis=1).loc[t3.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t4, pd.Series('Tier 4', index=t4.index)],\n",
    "                                axis=1).loc[t4.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t7, pd.Series('Tier 7', index=t7.index)],\n",
    "                                axis=1).loc[t7.map(len) > 0],\n",
    "                            pd.concat(\n",
    "                                [t7, pd.Series('', index=t7.index)],\n",
    "                                axis=1).loc[t7.map(len) == 0],\n",
    "                            ]).sort_index()\n",
    "    comb_tiers.columns = [series.name + '_UMLS', series.name + '_Tier']\n",
    "    # df_new = pd.concat([series, comb_tiers], axis=1)\n",
    "    df_new = comb_tiers\n",
    "\n",
    "    notfound = len(t7.loc[t7.map(len) == 0])\n",
    "\n",
    "    df_new[series.name + '_UMLS'] = pd.concat(\n",
    "        [series, df_new[series.name + '_UMLS'], t8], axis=1) \\\n",
    "        .apply(umls_filter, axis=1, result_type='reduce', string=True)\n",
    "\n",
    "    print(f'Done, {notfound} names not mapped')\n",
    "    return df_new\n",
    "\n",
    "df_tier = run_tiered_analysis(df[\"endpoint-name\"], linker, nlp, docs)\n",
    "df_comb = pd.concat([df[[\"endpoint-name\"]], df_tier], axis=1)\n",
    "print(df_comb.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
